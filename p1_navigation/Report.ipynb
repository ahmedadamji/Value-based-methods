{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the learning algorithm and implementation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithm being used here is the Vanilla Deep Q-Networks (DQN) algorithm, which is a model-free, off-policy reinforcement learning method. DQN learns an optimal policy by approximating the action-value function using a neural network. The DQN algorithm is implemented in the dqn() function of the Navigation.ipynb notebook. The implementation consists of an agent and an environment. The agent interacts with the environment by taking actions and receiving rewards. The agent's goal is to learn an optimal policy that maximizes the cumulative reward over time.\n",
    "\n",
    "The agent uses an epsilon-greedy exploration strategy to balance exploration and exploitation. The value of epsilon starts high at the beginning of the training and decays over time. The agent selects an action using the epsilon-greedy policy, which chooses a random action with probability epsilon, and the action with the highest Q-value with probability (1-epsilon).\n",
    "\n",
    "The implementation of the DQN algorithm in this project incorporates two important elements: Target Networks with Soft Updates and Experience Replay. Target networks are achieved by using two neural network architectures, one for the agent's local network and another for the target network. The agent's network receives the state as input and outputs the Q-values for all possible actions. The network is trained using the target network, which calculates the target Q-value of the current state-action pair using the estimated Q-value of the next state and the reward received. The loss is then calculated using mean squared error between the expected Q-value and the Q-value predicted by the network. The target network is periodically updated using the weights of the local network through soft updates. Experience replay is used to improve the stability and efficiency of the learning process by breaking correlations in the observation sequence and smoothing over changes in the data distribution. This is accomplished through the use of a replay buffer, which stores and samples experience tuples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture of the implemented neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network architecture used in this project is a simple feedforward neural network consisting of three fully connected (linear) layers with ReLU activation functions in between. The input layer has the same number of nodes as the state space dimension, which is 37 in this case. The first hidden layer (fc1) has 64 nodes, followed by the second hidden layer (fc2) which also has 64 nodes. The output layer has the same number of nodes as the action space dimension, which is 4 in this case. During forward pass, the input state vector is fed to the input layer (fc1), which outputs a 64-dimensional tensor. This tensor is then passed through the second hidden layer (fc2) which again outputs a 64-dimensional tensor. The output of fc2 is then passed through the output layer (fc3), which produces a 4-dimensional tensor representing the action values corresponding to each of the possible actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters used:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chosen hyperparameters were based on prior experience and experimentation, which showed that they lead to stable training and acceptable performance for the defined problem.\n",
    "\n",
    "n_episodes: The maximum number of training episodes. It is set to 2000.\n",
    "\n",
    "max_t: The maximum number of timesteps per episode. It is set to 1000.\n",
    "\n",
    "eps_start: The starting value of epsilon for the epsilon-greedy exploration strategy. It is set to 1.0, which means the agent initially takes random actions.\n",
    "\n",
    "eps_end: The minimum value of epsilon for the epsilon-greedy exploration strategy. It is set to 0.01, which means the agent will take the action with the highest Q-value 99% of the time.\n",
    "\n",
    "eps_decay: Factor for decreasing epsilon after each episode. It is set to 0.995, which means the agent will explore less as the number of episodes increases.\n",
    "\n",
    "\n",
    "#### Hyperparameters of the learning agent:\n",
    "\n",
    "buffer_size: The size of the replay buffer. In this implementation, it is set to 100,000. This value is used to store the experiences and samples are drawn from it to train the neural network.\n",
    "\n",
    "batch_size: The size of the mini-batch sampled from the replay buffer for each training iteration. In this implementation, it is set to 64.\n",
    "\n",
    "gamma: The discount factor used to calculate the future rewards. In this implementation, it is set to 0.99, which means the agent values immediate rewards more than future rewards.\n",
    "\n",
    "tau: The soft update parameter used to update the target network. In this implementation, it is set to 0.001, which means a slower update rate for the parameters of the target network to encourage a stable learning environment.\n",
    "\n",
    "lr: The learning rate used in the optimizer to update the parameters of the neural network. In this implementation, it is set to 0.0005.\n",
    "\n",
    "update_every: The number of time steps after which the agent updates the neural network. In this implementation, it is set to 4. This is to control the computational load of the algorithm.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f2f2f2; display: inline-block;\">\n",
    "    <img src=\"./Plot_of_Rewards.png\" alt=\"Plot of Rewards\" style=\"vertical-align: left;\">\n",
    "    <img src=\"./Training_Episodes.png\" alt=\"Plot of Rewards\" style=\"vertical-align: right;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, the environment was solved in 368 episodes with an average Score of 13.01."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the DQN algorithm, there are various modifications that can be applied. One approach is to implement additional extensions to the DQN algorithm, including:\n",
    "\n",
    "Double Q-learning: This method can be used to reduce the overestimation of Q-values that can occur with regular Q-learning. Double Q-learning uses two sets of weights to select and evaluate the action, which can help to reduce overoptimistic value estimates.\n",
    "\n",
    "Prioritized experience replay: Instead of sampling experiences uniformly, prioritized experience replay can be used to sample experiences that are more important for learning. This is achieved by assigning a priority to each experience based on its importance, which may be associated with a large associated TD error, and sampling experiences with higher priority more frequently.\n",
    "\n",
    "Dueling networks: Dueling networks can be used to separate the value and advantage functions, which allows for more efficient learning. By modifying the neural network architecture to have two streams, one for the value and one for the advantage, which are combined to give the final Q-value estimate.\n",
    "\n",
    "Multi-step bootstrapping targets: Multi-step bootstrapping targets can be used to look further ahead in time instead of only using the next state's Q-value to update the current state's Q-value. This can lead to faster and more efficient learning.\n",
    "\n",
    "Distributional DQN: Instead of estimating the expected value of the Q-function, distributional DQN models the distribution of the Q-values. This allows for a more accurate representation of the uncertainty in the value estimates and can lead to more robust learning.\n",
    "\n",
    "Noisy DQN: Noisy DQN adds noise to the weights of the neural network to encourage exploration and prevent the agent from getting stuck in local minima. This can help the agent converge to the global solution for the action values.\n",
    "\n",
    "Rainbow: Rainbow combines several of these techniques together, including double Q-learning, prioritized experience replay, dueling networks, multi-step bootstrapping targets, distributional DQN, and noisy DQN. Rainbow can create a highly effective reinforcement learning agent, which may significantly improve the agent's performance. However, tuning several hyperparameters may be necessary to achieve the best results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
